# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A5B3ynXlKAlFmkl0G5k_nJQ0qF98KV9n
"""

#uncomment to install
#pip install requests beautifulsoup4 pandas

#using drive as the source for input and target for output
from google.colab import drive
drive.mount ('/content/drive/')
import os
os.chdir('/content/drive/MyDrive/Python Datasets')
import pandas as pd
import requests
from bs4 import BeautifulSoup
import nltk
nltk.download('punkt')
from openpyxl import load_workbook
import string

#This block is for creating an empty output file
output_df = pd.DataFrame(columns=[
    'URL_ID',
    'URL',
    'POSITIVE SCORE',
    'NEGATIVE SCORE',
    'POLARITY SCORE',
    'SUBJECTIVITY SCORE',
    'AVG SENTENCE LENGTH',
    'PERCENTAGE OF COMPLEX WORDS',
    'FOG INDEX',
    'AVG NUMBER OF WORDS PER SENTENCE',
    'COMPLEX WORD COUNT',
    'WORD COUNT',
    'SYLLABLE PER WORD',
    'PERSONAL PRONOUNS',
    'AVG WORD LENGTH'
])

output_file = "Output Data Structure.xlsx"

output_df.to_excel(output_file, index=False)

print("File saved successfully.")

#Text analysis and Cleaning of code
data = pd.read_excel('Input.xlsx')
for index, row in data.iterrows():
    cleaned_text=""
    html_content=None
    response=None
    words_without_punctuation=""
    url = row['URL']
    url_id = row['URL_ID']

    response = requests.get(url)
    html_content = response.content
    words=""
    cleaned_words=""
    clean_count=0
    article_elements=""

# Create a BeautifulSoup object using the HTML content
    soup = BeautifulSoup(html_content, "html.parser")

# Find the article title
    title = soup.find("title").get_text()  # Replace "h1" with the appropriate selector for the article title

# Find the article text
    article_text = ""
    article_elements = soup.find_all("p")  # Replace "p" with the appropriate selector for the article paragraphs
    for element in article_elements:
      if not element.find("a"):
         article_text += element.get_text() + "\n"

    #Cleaning of Words
    words = article_text.split()
    folder_path = '/content/drive/MyDrive/Python Datasets/StopWords'
    file_names = os.listdir(folder_path)
    file_path=""
    cleaned=0

    for file_name in file_names:
        stopword_filename = file_name
        file_path = os.path.join(folder_path, file_name)
        encodings = ['utf-8', 'latin-1', 'utf-16']
        content = None

  # Access the file using the file path
        with open(file_path, 'r',encoding='latin-1') as file:
          content = file.read()
        words = article_text.split()

    # Remove stop words
        words = [word for word in words if word.lower() not in content]
    # Create a translation table with punctuation characters mapped to None
        translator = str.maketrans("", "", string.punctuation)

    # Remove punctuation from each word in the text
        words_without_punctuation = [word.translate(translator) for word in words]

    # Join the remaining words back into a cleaned text
        cleaned_text = ' '.join(words_without_punctuation)

     # Define the path to store the text files
    output_path = '/content/drive/MyDrive/Python Datasets/Outputfiles'

    # Create a file path with the output path and URL_ID as the file name
    file_path = os.path.join(output_path, f"{url_id}.txt")
    with open(file_path, 'w') as file:
        file.write("Title: ")
        file.write(title)
        file.write('\n')
        file.write("Content: ")
        file.write(cleaned_text)

    if os.path.exists(file_path):
        print(f"Article saved: {url_id}")
    else:
        print(f"Failed to save article: {url_id}")

    folder_path = '/content/drive/MyDrive/Python Datasets/MasterDictionary'
    file_names = os.listdir(folder_path)

    file_name = "positive-words.txt"
    file_path = os.path.join(folder_path, file_name)  # Replace with the path to your text file
    with open(file_path, 'r',encoding='utf-8') as file:

#positive_word_count
      df_positive = pd.read_csv(file_path, header=None, names=["Words"])
      positive_word_count=0
    for word in df_positive["Words"]:
       index=cleaned_text.lower().find(word)
       if(index!=-1):
        positive_word_count=positive_word_count+1
#negative_word_count
    folder_path = '/content/drive/MyDrive/Python Datasets/MasterDictionary'
    file_names = os.listdir(folder_path)

    file_name = "negative-words.txt"
    file_path = os.path.join(folder_path, file_name)  # Replace with the path to your text file
    with open(file_path, 'r') as file:
     df_negative = pd.read_csv(file_path, header=None, names=["Words"],encoding = "ISO-8859-1")
    negative_word_count=0
    for word in df_negative["Words"]:
      index=cleaned_text.lower().find(word)
      if(index!=-1):
       negative_word_count=negative_word_count-1
    negative_word_count=negative_word_count*-1
#Polarity Score
    #Polarity Score = (Positive Score â€“ Negative Score)/ ((Positive Score + Negative Score) + 0.000001)
    polarity_score=(positive_word_count-negative_word_count)/((positive_word_count+negative_word_count)+0.000001)
#Subjectivity Score
    #Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)
    cleaned_words=cleaned_text.split()
    clean_count=len(cleaned_words)
    sub_score=(positive_word_count+negative_word_count)/(clean_count++ 0.000001)
#Average Sentence Length
    sentences = nltk.sent_tokenize(cleaned_text)
    sen_len=len(sentences)
    #Average Sentence Length
    avg_sen_len=clean_count/sen_len
#Complex words
    def count_syllables(word):
    # Remove trailing "es" and "ed" from the word
       if word.endswith(("es", "ed")):
         word = word[:-2]
    # Count the number of vowels in the word
       vowels = "aeiou"
       count = sum(word.lower().count(vowel) for vowel in vowels)
       return count
    def find_complex_words(text):
    # Split the text into individual words
       words = text.split()
       complex_words = []
       for word in words:
        syllables = count_syllables(word)
        # Check if the syllable count is greater than 2
        if syllables > 2:
            complex_words.append(word)
       return complex_words
    complex_words = find_complex_words(cleaned_text)
#Percentage of Complex words
    complex_count=0
    complex_count = len(complex_words)
    percentage_complex_words=(complex_count)/clean_count
#Fog Index
    fog_index=0.4*(avg_sen_len+percentage_complex_words)
#average_words_per_sentence
    # Tokenize the text into sentences
    avg_words_per_sentence=0
    sentences = nltk.sent_tokenize(cleaned_text)
    # Count the total number of sentences and words
    num_sentences = len(sentences)
    num_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)
    # Calculate the average number of words per sentence
    avg_words_per_sentence = num_words / num_sentences
#Syllable Count per Word
    words = cleaned_text.split()
    syllable_counts = [count_syllables(word) for word in words]
#Personal Pronouns
    import re
    def count_personal_pronouns(text):
    # Define the regex pattern to match the personal pronouns
      pattern = r'\b(I|we|my|ours|us)\b'
    # Compile the regex pattern
      regex = re.compile(pattern, re.IGNORECASE)
    # Find all matches in the text
      matches = regex.findall(text)
    # Filter out 'US' from the matches
      matches = [match for match in matches if match.lower() != 'us']
    # Return the count of personal pronouns
      return len(matches)
    pronoun_count = count_personal_pronouns(cleaned_text)
#Average Word Length
    #Sum of the total number of characters in each word/Total number of words
    #words_without_punctuation # total number of words
    # Initialize an empty list to store the character counts per word
    character_counts = 0
    # Iterate over each word
    for word in words_without_punctuation:
    # Count the number of characters in the word
       character_count = len(word)
    # Append the character count to the list
       character_counts=character_counts+character_count
    # Return the list of character counts
    avg_word_length=character_counts/len(words_without_punctuation)
#saving all the values in Output file

    os.chdir('/content/drive/MyDrive/Python Datasets')
    # Open the existing workbook
    workbook = load_workbook('Output Data Structure.xlsx')

  # Select the active sheet
    sheet = workbook.active

  # Write the values into the sheet
    sheet.append([
        str(url_id),
        str(url),
    str(positive_word_count),
    str(negative_word_count),
    str(polarity_score),
    str(sub_score),
    str(avg_sen_len),
    str(percentage_complex_words),
    str(fog_index),
    str(avg_words_per_sentence),
    str(complex_count),
    str(clean_count),
    str(syllable_counts),
    str(pronoun_count),
    str(avg_word_length)
])

# Save the workbook
    workbook.save('Output Data Structure.xlsx')

    print("Data written successfully to the file.")


    soup.decompose()